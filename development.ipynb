{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d5b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging  \n",
    "from datetime import datetime    \n",
    "import time    \n",
    "from sqlalchemy import create_engine, text    \n",
    "from bs4 import BeautifulSoup    \n",
    "import requests    \n",
    "import polars as pl    \n",
    "import re    \n",
    "import pytz    \n",
    "import json    \n",
    "import pandas as pd    \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705bda48",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_yarn = os.environ.get('url_yarn')  \n",
    "mysql_access = os.environ.get('mysql_access')  \n",
    "interval = int(os.environ.get('interval'))  \n",
    "\n",
    "table_name = [\"test_yarn_all_resource\", \"test_yarn_list_jobs\", \"test_yarn_usage_groups\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cefed962",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scraping_all_yarn_resource():    \n",
    "    try:    \n",
    "        response = requests.get(url_yarn, timeout=10)    \n",
    "        response.raise_for_status()  # Check for HTTP errors    \n",
    "    except requests.exceptions.RequestException as e:    \n",
    "        logging.error(f\"Error while accessing the URL: {e}\")    \n",
    "        return None  # Return None if there's an issue with the request    \n",
    "        \n",
    "    # Parse HTML content    \n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")    \n",
    "    table = soup.find(\"table\", {\"id\": \"metricsoverview\"})    \n",
    "        \n",
    "    if table:    \n",
    "        # Extract headers    \n",
    "        headers = [header.text.strip() for header in table.find_all(\"th\")]    \n",
    "        \n",
    "        # Extract rows (skip the header row)    \n",
    "        rows = table.find_all(\"tr\")[1:]    \n",
    "        \n",
    "        # Parse the data into a list of dictionaries    \n",
    "        data = []    \n",
    "        for row in rows:    \n",
    "            cols = [col.text.strip() for col in row.find_all(\"td\")]    \n",
    "            if cols:    \n",
    "                data.append(dict(zip(headers, cols)))    \n",
    "        \n",
    "        for row in data:    \n",
    "            # Extract values from complex columns using regex    \n",
    "            used_match = re.search(r\"<memory:(\\d+(\\.\\d+)?)\\sGB, vCores:(\\d+)>\", row.get(\"Used Resources\", \"\"))    \n",
    "            total_match = re.search(r\"<memory:(\\d+(\\.\\d+)?)\\s(GB|TB), vCores:(\\d+)>\", row.get(\"Total Resources\", \"\"))    \n",
    "            reserved_match = re.search(r\"<memory:(\\d+)\\sB, vCores:(\\d+)>\", row.get(\"Reserved Resources\", \"\"))    \n",
    "        \n",
    "            # Convert units if necessary    \n",
    "            if used_match:    \n",
    "                row[\"Used Resources Vcpu\"] = int(used_match.group(3))    \n",
    "                row[\"Used Resources Memory\"] = float(used_match.group(1))    \n",
    "        \n",
    "            if total_match:    \n",
    "                memory_value = float(total_match.group(1))    \n",
    "                memory_unit = total_match.group(3)    \n",
    "                if memory_unit == \"TB\":    \n",
    "                    memory_value *= 1024  # Convert TB to GB    \n",
    "                row[\"Total Resources Memory GB\"] = int(memory_value)    \n",
    "                row[\"Total Resources Vcpu\"] = int(total_match.group(4))    \n",
    "        \n",
    "            if reserved_match:    \n",
    "                row[\"Reserved Resources Vcpu\"] = int(reserved_match.group(2))    \n",
    "                row[\"Reserved Resources Memory\"] = 0  # No memory information in this column    \n",
    "        \n",
    "        # Format data and ensure the necessary columns are available    \n",
    "        formatted_data = [    \n",
    "            {    \n",
    "                \"Apps Submitted\": int(row.get(\"Apps Submitted\", 0)),    \n",
    "                \"Apps Pending\": int(row.get(\"Apps Pending\", 0)),    \n",
    "                \"Apps Running\": int(row.get(\"Apps Running\", 0)),    \n",
    "                \"Apps Completed\": int(row.get(\"Apps Completed\", 0)),    \n",
    "                \"Containers Running\": int(row.get(\"Containers Running\", 0)),    \n",
    "                \"Used Resources Vcpu\": row.get(\"Used Resources Vcpu\", 0),    \n",
    "                \"Used Resources Memory GB\": row.get(\"Used Resources Memory\", 0),    \n",
    "                \"Total Resources Memory GB\": row.get(\"Total Resources Memory GB\", 0),    \n",
    "                \"Total Resources Vcpu\": row.get(\"Total Resources Vcpu\", 0),    \n",
    "                \"Physical Mem Used %\": int(row.get(\"Physical Mem Used %\", 0)),    \n",
    "                \"Physical VCores Used %\": int(row.get(\"Physical VCores Used %\", 0)),    \n",
    "            }    \n",
    "            for row in data    \n",
    "        ]    \n",
    "        \n",
    "        return pl.DataFrame(formatted_data)    \n",
    "    else:    \n",
    "        logging.warning(\"Table not found on the page.\")    \n",
    "        return None    \n",
    "    \n",
    "def scraping_yarn_job_running():    \n",
    "    try:    \n",
    "        response = requests.get(url_yarn+\"/apps/RUNNING\", timeout=10)    \n",
    "        response.raise_for_status()  # Check for HTTP errors    \n",
    "    except requests.exceptions.RequestException as e:    \n",
    "        logging.error(f\"Error while accessing the URL: {e}\")    \n",
    "        return None  # Return None if there's an issue with the request    \n",
    "        \n",
    "    # Parse HTML content    \n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")    \n",
    "        \n",
    "    # Find data table in JavaScript    \n",
    "    script = soup.find(\"script\", string=lambda t: \"var appsTableData=\" in t)    \n",
    "    if script:    \n",
    "        # Extract data from JavaScript    \n",
    "        table_data = script.string.split(\"var appsTableData=\")[-1].strip()    \n",
    "        table_data = table_data.split(\";\")[0]    \n",
    "            \n",
    "        # Convert data to Python list    \n",
    "        apps_data = json.loads(table_data)    \n",
    "            \n",
    "        # Define table columns    \n",
    "        columns = [    \n",
    "            \"ID\", \"User\", \"Name\", \"Application Type\", \"Application Tags\",    \n",
    "            \"Queue\", \"Application Priority\", \"Start Time\", \"Launch Time\",    \n",
    "            \"Finish Time\", \"State\", \"Final Status\", \"Running Containers\",    \n",
    "            \"Allocated CPU VCores\", \"Allocated Memory MB\", \"Allocated GPUs\",    \n",
    "            \"Reserved CPU VCores\", \"Reserved Memory MB\", \"Reserved GPUs\",    \n",
    "            \"% of Queue\", \"% of Cluster\", \"Progress\", \"Tracking UI\", \"Blacklisted Nodes\"    \n",
    "        ]    \n",
    "            \n",
    "        # Create DataFrame    \n",
    "        df = pd.DataFrame(apps_data, columns=columns)    \n",
    "            \n",
    "        # Function to extract ID from ID column    \n",
    "        def extract_id(id_str):    \n",
    "            match = re.search(r'application_\\d+_\\d+', id_str)    \n",
    "            return match.group(0) if match else None    \n",
    "            \n",
    "        # Function to extract URL from Tracking UI column    \n",
    "        def extract_tracking_ui(url_str):    \n",
    "            match = re.search(r\"'(http://[^']+)'\", url_str)    \n",
    "            return match.group(1) if match else None    \n",
    "            \n",
    "        # Apply functions to ID and Tracking UI columns    \n",
    "        df['ID'] = df['ID'].apply(extract_id)    \n",
    "        df['Tracking UI'] = df['Tracking UI'].apply(extract_tracking_ui)    \n",
    "            \n",
    "        df['Start Time'] = pd.to_datetime(df['Start Time'].astype(float), unit='ms')    \n",
    "        df['Launch Time'] = pd.to_datetime(df['Launch Time'].astype(float), unit='ms')    \n",
    "        df['Finish Time'] = pd.to_datetime(df['Finish Time'].astype(float), unit='ms')    \n",
    "            \n",
    "        # Drop Progress column    \n",
    "        df.drop(columns=['Progress'], inplace=True)    \n",
    "            \n",
    "        # Rename columns    \n",
    "        df = df.rename(columns={    \n",
    "            \"ID\": \"id\",    \n",
    "            \"Name\": \"name\",    \n",
    "            \"Application Type\": \"application_type\",    \n",
    "            \"Running Containers\": \"running_containers\",    \n",
    "            \"Allocated CPU VCores\": \"allocated_cpu_vcores\",    \n",
    "            \"Allocated Memory MB\": \"allocated_memory_mb\",    \n",
    "            \"Reserved CPU VCores\": \"reserved_cpu_vcores\",    \n",
    "            \"Reserved Memory MB\": \"reserved_memory_mb\",    \n",
    "            \"% of Cluster\": \"usage_of_cluster\",    \n",
    "            \"Tracking UI\": \"tracking_ui\"    \n",
    "        })    \n",
    "            \n",
    "        # Convert columns to numeric types with default value of 0 for null values\n",
    "        df['running_containers'] = pd.to_numeric(df['running_containers'], errors='coerce').fillna(0).astype(int)\n",
    "        df['allocated_cpu_vcores'] = pd.to_numeric(df['allocated_cpu_vcores'], errors='coerce').fillna(0).astype(int)\n",
    "        df['allocated_memory_mb'] = pd.to_numeric(df['allocated_memory_mb'], errors='coerce').fillna(0).astype(int)\n",
    "        df['reserved_cpu_vcores'] = pd.to_numeric(df['reserved_cpu_vcores'], errors='coerce').fillna(0).astype(int)\n",
    "        df['reserved_memory_mb'] = pd.to_numeric(df['reserved_memory_mb'], errors='coerce').fillna(0).astype(int)\n",
    "        df['usage_of_cluster'] = pd.to_numeric(df['usage_of_cluster'], errors='coerce').fillna(0).astype(float)\n",
    "        # Split the \"name\" column and take the first part as \"prefix\"\n",
    "        df['prefix'] = df['name'].str.split(\"-\").str[0].str.strip().fillna('')\n",
    "            \n",
    "        return df  \n",
    "    else:    \n",
    "        logging.warning(\"Data table not found.\")    \n",
    "        return None    \n",
    "  \n",
    "def calculate_group_usages():  \n",
    "    df_list_job = scraping_yarn_job_running()  \n",
    "    if df_list_job is None:  \n",
    "        return None  \n",
    "  \n",
    "    # Group by prefix and sum the relevant columns    \n",
    "    df_usage_groups = df_list_job.groupby(\"prefix\")[[\"allocated_cpu_vcores\", \"allocated_memory_mb\", \"reserved_cpu_vcores\", \"reserved_memory_mb\", \"usage_of_cluster\"]].sum().reset_index()  \n",
    "          \n",
    "    all_prefixes = ['BDA', 'GG', 'SM']  \n",
    "  \n",
    "    # DataFrame for dummy rows  \n",
    "    dummy_rows = []  \n",
    "  \n",
    "    # Loop to check each prefix  \n",
    "    for prefix in all_prefixes:  \n",
    "        if prefix not in df_usage_groups['prefix'].values:  \n",
    "            dummy_row = {  \n",
    "                'prefix': prefix,  \n",
    "                'allocated_cpu_vcores': 0,  \n",
    "                'allocated_memory_mb': 0,  \n",
    "                'reserved_cpu_vcores': 0,  \n",
    "                'reserved_memory_mb': 0,  \n",
    "                'usage_of_cluster': 0,  \n",
    "            }  \n",
    "            dummy_rows.append(dummy_row)  \n",
    "  \n",
    "    # If there are new prefixes, add to DataFrame  \n",
    "    if dummy_rows:  \n",
    "        df_usage_groups = pd.concat([df_usage_groups, pd.DataFrame(dummy_rows)], ignore_index=True)  \n",
    "  \n",
    "    return df_usage_groups  \n",
    "  \n",
    "engine = create_engine(f\"mysql+pymysql://{mysql_access}\")      \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c0bcf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_resource = scraping_all_yarn_resource()    \n",
    "if df_all_resource is not None:    \n",
    "    # Add the current UTC date to the DataFrame    \n",
    "    df_all_resource_utc = df_all_resource.with_columns(    \n",
    "        pl.lit(datetime.now(pytz.UTC)).alias(\"date\")    \n",
    "    )    \n",
    "    try:    \n",
    "        # Polars  \n",
    "        df_all_resource_utc.write_database(    \n",
    "            table_name=table_name[0],    \n",
    "            connection=engine,    \n",
    "            if_table_exists=\"append\",  \n",
    "        )    \n",
    "        logging.info(f\"All Resource data: '{df_all_resource.shape[0]} rows' inserted into '{table_name[0]}'.\")    \n",
    "    except Exception as e:    \n",
    "        logging.error(f\"Error while writing resource data to database: {e}\")    \n",
    "        # Scrape Running Jobs Yarn  \n",
    "df_list_job = scraping_yarn_job_running()  \n",
    "if df_list_job is not None:  \n",
    "    try:\n",
    "        # Delete all row in table using proper SQLAlchemy syntax\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(f\"DELETE FROM {table_name[1]}\"))\n",
    "            conn.commit()  # Add commit to ensure changes are saved\n",
    "            \n",
    "        # Insert Data\n",
    "        df_list_job.to_sql(  \n",
    "            name=table_name[1],   \n",
    "            con=engine,   \n",
    "            if_exists='append',   \n",
    "            index=False\n",
    "        )    \n",
    "        \n",
    "        logging.info(f\"List job yarn : '{len(df_list_job)} rows' inserted into '{table_name[1]}'.\")    \n",
    "    except Exception as e:    \n",
    "        logging.error(f\"Error while writing 'list job yarn' to database: {e}\")  \n",
    "# Scrape Calculation Jobs Yarn  \n",
    "df_usage_groups = calculate_group_usages()  \n",
    "if df_usage_groups is not None:    \n",
    "    try:    \n",
    "        # Pandas  \n",
    "        df_usage_groups['date'] = pd.Timestamp.now(pytz.UTC)  \n",
    "        df_usage_groups.to_sql(  \n",
    "            name=table_name[2],   \n",
    "            con=engine,  \n",
    "            if_exists='append',   \n",
    "            index=False  # Avoid including index column  \n",
    "        )    \n",
    "        logging.info(f\"Usage Group Data : '{len(df_usage_groups)} rows' inserted into '{table_name[2]}'.\")    \n",
    "    except Exception as e:    \n",
    "        logging.error(f\"Error while writing application data to database: {e}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620ce20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while True:              \n",
    "    df_all_resource = scraping_all_yarn_resource()    \n",
    "    if df_all_resource is not None:    \n",
    "        # Add the current UTC date to the DataFrame    \n",
    "        df_all_resource_utc = df_all_resource.with_columns(    \n",
    "            pl.lit(datetime.now(pytz.UTC)).alias(\"date\")    \n",
    "        )    \n",
    "        try:    \n",
    "            # Polars  \n",
    "            df_all_resource_utc.write_database(    \n",
    "                table_name=table_name[0],    \n",
    "                connection=engine,    \n",
    "                if_table_exists=\"append\",  \n",
    "            )    \n",
    "            logging.info(f\"All Resource data: '{df_all_resource.shape[0]} rows' inserted into '{table_name[0]}'.\")    \n",
    "        except Exception as e:    \n",
    "            logging.error(f\"Error while writing resource data to database: {e}\")    \n",
    "          \n",
    "    # Scrape Running Jobs Yarn  \n",
    "    df_list_job = scraping_yarn_job_running()  \n",
    "    if df_list_job is not None:  \n",
    "        try:    \n",
    "            # Pandas  \n",
    "            df_list_job.to_sql(  \n",
    "                name=table_name[1],   \n",
    "                con=engine,   \n",
    "                if_exists='replace',   \n",
    "                index=False  # for overwrite  \n",
    "            )    \n",
    "            logging.info(f\"List job yarn : '{len(df_list_job)} rows' inserted into '{table_name[1]}'.\")    \n",
    "        except Exception as e:    \n",
    "            logging.error(f\"Error while writing 'list job yarn' to database: {e}\")  \n",
    "          \n",
    "    # Scrape Calculation Jobs Yarn  \n",
    "    df_usage_groups = calculate_group_usages()  \n",
    "    if df_usage_groups is not None:    \n",
    "        try:    \n",
    "            # Pandas  \n",
    "            df_usage_groups['date'] = pd.Timestamp.now(pytz.UTC)  \n",
    "            df_usage_groups.to_sql(  \n",
    "                name=table_name[2],   \n",
    "                con=engine,  \n",
    "                if_exists='append',   \n",
    "                index=False  # Avoid including index column  \n",
    "            )    \n",
    "            logging.info(f\"Usage Group Data : '{len(df_usage_groups)} rows' inserted into '{table_name[2]}'.\")    \n",
    "        except Exception as e:    \n",
    "            logging.error(f\"Error while writing application data to database: {e}\")  \n",
    "      \n",
    "    time.sleep(interval)  \n",
    "      \n",
    "# Usage: python main.py yarn_all_resource_test yarn_list_jobs yarn_usage_groups  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
